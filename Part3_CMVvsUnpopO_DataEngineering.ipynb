{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d7c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt import BayesSearchCV\n",
    "from scipy.stats import uniform, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4fa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(tn, fp, fn, tp, metric = ['accuracy']):\n",
    "    answers = {}\n",
    "    \n",
    "    if 'accuracy' in metric or 'all' in metric:\n",
    "        answers['accuracy'] = (tp + tn) / (tn + fn + fp + tp)\n",
    "    if 'sensitivity' in metric  or 'all' in metric:\n",
    "        answers['sensitivity'] = tp / (tp + fn)\n",
    "    if 'specificity' in metric  or 'all' in metric:\n",
    "        answers['specificity'] = tn / (tn + fp)\n",
    "    if 'f1' in metric or 'all' in metric:\n",
    "        answers['f1'] = tp / (tp + .5*(fp + fn))\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73923c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned_cmv&unpop_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543dcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e72fb1",
   "metadata": {},
   "source": [
    "***Data Engineering***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597279f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unpopularopinion    0.563229\n",
       "changemyview        0.436771\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d13c9e",
   "metadata": {},
   "source": [
    "- Set changemyview to be the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898ce55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map(lambda x: 1 if x == 'changemyview' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9137b6",
   "metadata": {},
   "source": [
    "- Scrub URLs and CMVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4210c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_text(string):\n",
    "    remove_cmv_url = r'^([Cc]{1}[Mm]{1}[Vv]{1}[:]{0,1})|([Ww]{3}[^\\s]+)|([^\\s]+[\\.]{1}[Cc]{1}[Oo]{1}[Mm]{1})$'\n",
    "    return re.sub(remove_cmv_url, '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed0873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(scrub_text)\n",
    "df['selftext'] = df['selftext'].map(scrub_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f754d",
   "metadata": {},
   "source": [
    "- Remove inserted moderator comments. Many change my view posts have \"_____ gt hello user of cmv this is a footnote...[etc.]\" or similar appended to the end of their selftext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c227b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_mod_comment(string):\n",
    "    return string.split('_____ gt')[0].split('gt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4b475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].map(scrub_mod_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441cd4f",
   "metadata": {},
   "source": [
    "- Lemmatize text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcedfa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46af874",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "583124b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_map(in_string):  \n",
    "    to_lemma = tokenizer.tokenize(in_string)\n",
    "    return \" \".join([lemmatizer.lemmatize(token.lower()) for token in to_lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063b8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(lemma_map)\n",
    "df['selftext'] = df['selftext'].map(lemma_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01cc0a",
   "metadata": {},
   "source": [
    "- Add sentiment analysis columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5bec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = zip(list(df['title']), list(df['selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b2f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc8e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []    \n",
    "\n",
    "for title, tweet in corpus:\n",
    "    scores = sia.polarity_scores(title + \" \" + tweet)\n",
    "    sentiment.append(scores)\n",
    "\n",
    "sents = pd.DataFrame(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb068e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, sents], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79d736",
   "metadata": {},
   "source": [
    "- Change column names so that there won't be a collision with vectorized columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe254cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'author': 'author_username', 'id': 'author_id', 'selftext': 'post_text', 'score': 'post_score', 'subreddit': 'post_subreddit', 'title': 'post_title', 'neg': 'neg_sentiment', 'pos': 'pos_sentiment', 'neu': 'neu_sentiment', 'compound': 'comp_sentiment'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab13f24",
   "metadata": {},
   "source": [
    "- Add training column for fitting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d6f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = df.columns.tolist()\n",
    "x_list.remove('post_subreddit')\n",
    "X = df[x_list]\n",
    "y = df['post_subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d19f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = pd.concat([X_train, y_train], axis = 1)\n",
    "test_total = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499fe8",
   "metadata": {},
   "source": [
    "- Identify best parameters for vectorizer. Code credit to lesson 'advanced hyperparameter search'\n",
    "- Multinomial Naive Bayes is used as a fast, easily optimized model that works well on natural language data to pipe our vectorizer through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words = 'english')),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cvec__max_features': Integer(100, 10000),\n",
    "    'cvec__min_df': Integer(1, 5),\n",
    "    'cvec__max_df': Real(.5,1, prior='uniform'),\n",
    "    'mnb__alpha': Real(.001, 100, prior='log-uniform')\n",
    "}\n",
    "\n",
    "cvec_bs = BayesSearchCV(estimator = pipe,\n",
    "                     search_spaces = params,\n",
    "                     scoring = 'f1',\n",
    "                     n_iter = 50,\n",
    "                     n_jobs = 8,\n",
    "                     cv = 5,\n",
    "                     refit = True,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee38b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_post = X_train['post_title'] + \" \" + X_train['post_text']\n",
    "test_text_post = X_test['post_title'] + \" \" + X_test['post_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d34a212a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=Pipeline(steps=[('cvec',\n",
       "                                         CountVectorizer(stop_words='english')),\n",
       "                                        ('mnb', MultinomialNB())]),\n",
       "              n_jobs=8, random_state=42, scoring='f1',\n",
       "              search_spaces={'cvec__max_df': Real(low=0.5, high=1, prior='uniform', transform='identity'),\n",
       "                             'cvec__max_features': Integer(low=100, high=10000, prior='uniform', transform='identity'),\n",
       "                             'cvec__min_df': Integer(low=1, high=5, prior='uniform', transform='identity'),\n",
       "                             'mnb__alpha': Real(low=0.001, high=100, prior='log-uniform', transform='identity')})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_bs.fit(train_text_post, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8717187e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cvec__max_df', 0.8468466496240046),\n",
       "             ('cvec__max_features', 1063),\n",
       "             ('cvec__min_df', 1),\n",
       "             ('mnb__alpha', 0.001)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_bs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f126c6",
   "metadata": {},
   "source": [
    "- Train performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a5358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cvec_bs.predict(train_text_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ee90752",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_train, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0393079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7966140471288035,\n",
       " 'sensitivity': 0.8017286537454165,\n",
       " 'specificity': 0.7926482534524777,\n",
       " 'f1': 0.7749367088607595}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(tn, fp, fn, tp, metric = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39076fca",
   "metadata": {},
   "source": [
    "- Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3e0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cvec_bs.predict(test_text_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5675aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "467c23fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7848318462594372,\n",
       " 'sensitivity': 0.7894736842105263,\n",
       " 'specificity': 0.7812309567336989,\n",
       " 'f1': 0.7622298065984073}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(tn, fp, fn, tp, metric = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d56cf9",
   "metadata": {},
   "source": [
    "- Vectorize words and add to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc9aa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = cvec_bs.best_estimator_['cvec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "927819b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcv_train = cvec.fit_transform(train_total['post_title'] + \" \" + train_total['post_text'])\n",
    "Xcv_test = cvec.transform(test_total['post_title'] + \" \" + test_total['post_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "360e592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xcv_train = pd.DataFrame(Xcv_train.todense(), columns = cvec.get_feature_names())\n",
    "df_xcv_test = pd.DataFrame(Xcv_test.todense(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6febb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xcv_train.reset_index(inplace = True)\n",
    "df_xcv_test.reset_index(inplace = True)\n",
    "train_total.reset_index(inplace = True)\n",
    "test_total.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caeed565",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_vecs = pd.concat([train_total, df_xcv_train], axis = 1)\n",
    "test_with_vecs = pd.concat([test_total, df_xcv_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23b448e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_with_vecs, test_with_vecs], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95d593f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "846ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'index', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5436135",
   "metadata": {},
   "source": [
    "- Save engineered file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f4d1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/engineered_cmv&unpop_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2f760",
   "metadata": {},
   "source": [
    "- On to part 4 ->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
